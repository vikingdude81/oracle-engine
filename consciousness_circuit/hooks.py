"""\nLightweight hooks and caching utilities for transformer models.\n\nGoals:\n- Grab hidden states / attentions via forward pass (no model surgery).\n- Provide best-effort layer stack discovery for common HF models.\n- Optional logit-lens helper on cached hidden states.\n"""\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport torch\n\n\n@dataclass\nclass CaptureResult:\n    tokens: List[str]\n    token_ids: torch.Tensor\n    logits: torch.Tensor\n    hidden_states: List[torch.Tensor]\n    attentions: Optional[List[torch.Tensor]]\n\n    def to_cpu(self) -> "CaptureResult":  # type: ignore\n        return CaptureResult(\n            tokens=self.tokens,\n            token_ids=self.token_ids.cpu(),\n            logits=self.logits.cpu(),\n            hidden_states=[h.cpu() for h in self.hidden_states],\n            attentions=[a.cpu() for a in self.attentions] if self.attentions else None,\n        )\n\n\ndef _find_layer_stack(model: torch.nn.Module) -> List[torch.nn.Module]:\n    """Best-effort discovery of transformer block stack."""\n    candidates = [\n        getattr(model, "model", None),\n        getattr(model, "transformer", None),\n        getattr(model, "decoder", None),\n        model,\n    ]\n    for root in candidates:\n        if root is None:\n            continue\n        for attr in ("layers", "h", "blocks", "encoder_layer", "layer"):\n            stack = getattr(root, attr, None)\n            if isinstance(stack, (list, tuple)) and len(stack) > 0:\n                return list(stack)\n            if hasattr(stack, "__len__") and len(stack) > 0:\n                return list(stack)\n    raise ValueError("Could not find transformer layer stack (layers/h/blocks)")\n\n\ndef capture_forward(\n    model: torch.nn.Module,\n    tokenizer: Any,\n    prompt: str,\n    device: Optional[str] = None,\n    max_new_tokens: int = 0,\n    output_attentions: bool = True,\n) -> CaptureResult:\n    """Run a forward pass and cache hidden states/attentions.\n\n    Note: this does not generate text; it runs a single forward with the prompt tokens.\n    """\n    model_device = next(model.parameters()).device\n    device = device or str(model_device)\n\n    encoded = tokenizer(prompt, return_tensors="pt").to(device)\n    with torch.no_grad():\n        out = model(\n            **encoded,\n            output_hidden_states=True,\n            output_attentions=output_attentions,\n            use_cache=False,\n        )\n    hidden_states = list(out.hidden_states)  # type: ignore\n    attentions = list(out.attentions) if output_attentions and hasattr(out, "attentions") else None\n    logits = out.logits  # type: ignore\n    tokens = tokenizer.convert_ids_to_tokens(encoded.input_ids[0])\n    return CaptureResult(tokens=tokens, token_ids=encoded.input_ids[0], logits=logits, hidden_states=hidden_states, attentions=attentions)\n\n\ndef logit_lens(\n    hidden_states: List[torch.Tensor],\n    model: torch.nn.Module,\n    tokenizer: Any,\n    top_k: int = 5,\n) -> List[List[Tuple[str, float]]]:\n    """Apply a simple logit lens: project intermediate hidden states to token probs."""\n    if not hasattr(model, "lm_head"):\n        raise ValueError("Model has no lm_head for logit lens")\n    results: List[List[Tuple[str, float]]] = []\n    device = hidden_states[0].device\n    lm_head = model.lm_head\n    norm = getattr(model, "model", None)\n    norm = getattr(norm, "norm", None) if norm else None\n\n    for hs in hidden_states:\n        x = hs\n        if x.dim() == 3:\n            x_last = x[:, -1, :]\n        else:\n            x_last = x\n        if norm:\n            x_last = norm(x_last)\n        logits = lm_head(x_last)  # type: ignore\n        probs = torch.softmax(logits, dim=-1)[0]\n        topv, topi = torch.topk(probs, k=top_k)\n        results.append([(tokenizer.decode([i.item()]), v.item()) for v, i in zip(topv, topi)])\n    return results\n\n\ndef apply_residual_steering(\n    model: torch.nn.Module,\n    layer_idx: int,\n    direction: torch.Tensor,\n    alpha: float = 1.0,\n    token_pos: int = -1,\n) -> torch.utils.hooks.RemovableHandle:\n    """Add a steering vector to the residual stream at a given layer output."""\n    layers = _find_layer_stack(model)\n    target = layers[layer_idx]\n    direction = direction.to(next(model.parameters()).device)\n\n    def hook(_module, _inp, out):\n        if not torch.is_tensor(out):\n            return out\n        if out.dim() == 3:\n            out = out.clone()\n            out[:, token_pos, :] += alpha * direction\n            return out\n        return out\n\n    return target.register_forward_hook(hook)\n