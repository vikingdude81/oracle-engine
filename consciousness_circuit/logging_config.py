"""\nCentralized Logging Configuration for Consciousness Circuit\n============================================================\n\nProvides consistent logging across all modules with:\n- File + console output\n- Configurable log levels\n- Structured format with timestamps\n- Easy integration\n\nUsage:\n    from consciousness_circuit.logging_config import get_logger\n\n    logger = get_logger(__name__)\n    logger.info("Starting experiment")\n    logger.warning("Low memory")\n    logger.error("Failed to load model", exc_info=True)\n"""\n\nimport logging\nimport sys\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import Optional\n\n\n# Global configuration\nDEFAULT_LOG_LEVEL = logging.INFO\nLOG_FORMAT = "%(asctime)s | %(levelname)-8s | %(name)s | %(message)s"\nDATE_FORMAT = "%Y-%m-%d %H:%M:%S"\n\n# Track if logging has been configured\n_logging_configured = False\n_log_file_path: Optional[Path] = None\n\n\ndef setup_logging(\n    level: int = DEFAULT_LOG_LEVEL,\n    log_file: Optional[str] = None,\n    console: bool = True,\n) -> Path:\n    """\n    Configure logging for the entire consciousness_circuit package.\n\n    Args:\n        level: Logging level (logging.DEBUG, INFO, WARNING, ERROR)\n        log_file: Path to log file (default: auto-generated in logs/)\n        console: Whether to also log to console\n\n    Returns:\n        Path to the log file\n    """\n    global _logging_configured, _log_file_path\n\n    # Create logs directory\n    logs_dir = Path(__file__).parent.parent / "logs"\n    logs_dir.mkdir(exist_ok=True)\n\n    # Generate log file name if not provided\n    if log_file is None:\n        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")\n        log_file = logs_dir / f"consciousness_{timestamp}.log"\n    else:\n        log_file = Path(log_file)\n\n    _log_file_path = log_file\n\n    # Get root logger for consciousness_circuit\n    root_logger = logging.getLogger("consciousness_circuit")\n    root_logger.setLevel(level)\n\n    # Remove existing handlers\n    root_logger.handlers.clear()\n\n    # Create formatter\n    formatter = logging.Formatter(LOG_FORMAT, DATE_FORMAT)\n\n    # File handler\n    file_handler = logging.FileHandler(log_file, encoding='utf-8')\n    file_handler.setLevel(level)\n    file_handler.setFormatter(formatter)\n    root_logger.addHandler(file_handler)\n\n    # Console handler\n    if console:\n        console_handler = logging.StreamHandler(sys.stdout)\n        console_handler.setLevel(level)\n        console_handler.setFormatter(formatter)\n        root_logger.addHandler(console_handler)\n\n    _logging_configured = True\n\n    root_logger.info(f"Logging initialized: {log_file}")\n\n    return log_file\n\n\ndef get_logger(name: str) -> logging.Logger:\n    """\n    Get a logger for a specific module.\n\n    Args:\n        name: Module name (typically __name__)\n\n    Returns:\n        Configured logger instance\n\n    Example:\n        logger = get_logger(__name__)\n        logger.info("Processing started")\n    """\n    global _logging_configured\n\n    # Auto-configure if not done yet\n    if not _logging_configured:\n        setup_logging()\n\n    # Ensure name is under consciousness_circuit namespace\n    if not name.startswith("consciousness_circuit"):\n        name = f"consciousness_circuit.{name}"\n\n    return logging.getLogger(name)\n\n\ndef log_exception(logger: logging.Logger, msg: str, exc: Exception) -> None:\n    """\n    Log an exception with full traceback.\n\n    Args:\n        logger: Logger instance\n        msg: Context message\n        exc: The exception\n    """\n    logger.error(f"{msg}: {type(exc).__name__}: {exc}", exc_info=True)\n\n\ndef log_gpu_memory(logger: logging.Logger) -> None:\n    """Log current GPU memory usage."""\n    try:\n        import torch\n        if torch.cuda.is_available():\n            allocated = torch.cuda.memory_allocated() / 1024**3\n            reserved = torch.cuda.memory_reserved() / 1024**3\n            logger.debug(f"GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved")\n    except ImportError:\n        pass\n\n\nclass ExperimentLogger:\n    """\n    Context manager for experiment logging with timing.\n\n    Usage:\n        with ExperimentLogger("layerwise_analysis") as exp:\n            exp.log("Starting analysis")\n            # ... do work ...\n            exp.log("Found peak at layer 42")\n    """\n\n    def __init__(self, name: str, logger: Optional[logging.Logger] = None):\n        self.name = name\n        self.logger = logger or get_logger(f"experiment.{name}")\n        self.start_time = None\n\n    def __enter__(self):\n        import time\n        self.start_time = time.time()\n        self.logger.info(f"=== Starting: {self.name} ===")\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        import time\n        elapsed = time.time() - self.start_time\n\n        if exc_type is not None:\n            self.logger.error(\n                f"=== Failed: {self.name} ({elapsed:.1f}s) ===",\n                exc_info=(exc_type, exc_val, exc_tb)\n            )\n            return False  # Re-raise exception\n        else:\n            self.logger.info(f"=== Completed: {self.name} ({elapsed:.1f}s) ===")\n            return True\n\n    def log(self, msg: str, level: int = logging.INFO):\n        """Log a message within this experiment context."""\n        self.logger.log(level, f"[{self.name}] {msg}")\n\n    def debug(self, msg: str):\n        self.log(msg, logging.DEBUG)\n\n    def warning(self, msg: str):\n        self.log(msg, logging.WARNING)\n\n    def error(self, msg: str):\n        self.log(msg, logging.ERROR)\n\n\n# Convenience function for quick setup\ndef quick_setup(verbose: bool = False) -> logging.Logger:\n    """\n    Quick setup for scripts.\n\n    Args:\n        verbose: If True, use DEBUG level; otherwise INFO\n\n    Returns:\n        Root logger for consciousness_circuit\n    """\n    level = logging.DEBUG if verbose else logging.INFO\n    setup_logging(level=level)\n    return get_logger("consciousness_circuit")\n